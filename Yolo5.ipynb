{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "In this Jupyter notebook, I demonstrate the process of training an object detection model for car detection using YOLOv5. The main goal is to fine-tune the pre-trained YOLOv5 model on a custom dataset of car images to improve its performance for detecting cars in images specific to the use case.\n",
        "\n",
        "The notebook is organized into the following steps:\n",
        "\n",
        "1. Preparation of the custom car dataset: Splitting the dataset into training and validation sets, and converting the annotations into a format suitable for use with YOLOv5.\n",
        "2. Training YOLOv5 on the custom car dataset: Running the training script with the necessary parameters, such as image size, batch size, number of epochs, dataset configuration file, pre-trained weights, and the name of the training run.\n",
        "3. Validating the trained YOLOv5 model: Running the validation script with the necessary parameters, such as dataset configuration file, trained weights, image size, IOU threshold, and whether to use half-precision inference.\n",
        "\n",
        "By following these steps, I aim to achieve a better-performing car detection model fine-tuned for the specific use case.\n"
      ],
      "metadata": {
        "id": "3UK5USPIucdC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoCrpZ55OAUN",
        "outputId": "a4150fe7-ef3b-4006-9afc-38235dc310b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd gdrive/MyDrive/Datrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiNPxkaKOA15",
        "outputId": "ecbdfe4b-9c75-41e5-ead8-d3e311a8c35d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Datrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YDkOdJ_OB6P",
        "outputId": "5f6b1407-a3e1-4aeb-d349-6998a9914408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'yolov5' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd yolov5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFmBeIHOOG4H",
        "outputId": "374167c1-7692-4ff1-e9c4-72e38dd35811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Datrix/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6I4luNzOHiP",
        "outputId": "f8ebd0c7-3f07-4a5d-e004-0250f62ce3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gitpython>=3.1.30\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 6)) (3.5.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 7)) (1.22.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 8)) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 9)) (8.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 10)) (5.4.8)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 11)) (6.0)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 12)) (2.25.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 13)) (1.10.1)\n",
            "Collecting thop>=0.1.1\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 15)) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 16)) (0.14.1+cu116)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 17)) (4.65.0)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 21)) (2.11.2)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 26)) (1.4.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 27)) (0.11.2)\n",
            "Collecting setuptools>=65.5.1\n",
            "  Downloading setuptools-67.6.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (4.39.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (1.4.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (1.26.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (4.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->-r requirements.txt (line 15)) (4.5.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (2.16.2)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.19.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (2.2.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.51.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.4.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 26)) (2022.7.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 21)) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->-r requirements.txt (line 21)) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.2.2)\n",
            "Installing collected packages: smmap, setuptools, thop, gitdb, gitpython\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 63.4.3\n",
            "    Uninstalling setuptools-63.4.3:\n",
            "      Successfully uninstalled setuptools-63.4.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 67.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gitdb-4.0.10 gitpython-3.1.31 setuptools-67.6.0 smmap-5.0.0 thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import cv2"
      ],
      "metadata": {
        "id": "f3yyGSM5OJP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the annotations file\n",
        "with open('../Data/cars/annotations_sample.json') as json_file:\n",
        "    annotations = json.load(json_file)\n",
        "\n",
        "# Create the necessary directories\n",
        "os.makedirs('../Data/car/train', exist_ok=True)\n",
        "os.makedirs('../Data/car/val', exist_ok=True)\n",
        "os.makedirs('../Data/car/labels/train', exist_ok=True)\n",
        "os.makedirs('../Data/car/labels/val', exist_ok=True)"
      ],
      "metadata": {
        "id": "Vd5yrZGGOVVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split ratio for train and val datasets\n",
        "split_ratio = 0.8\n",
        "train_size = int(len(annotations['annotations']) * split_ratio)"
      ],
      "metadata": {
        "id": "jhFcLHn9OXpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_bbox_to_yolo(bbox, img_width, img_height):\n",
        "    \"\"\"\n",
        "      Convert a bounding box from the format (x_min, y_min, width, height) to\n",
        "      the YOLO format (x_center, y_center, width, height), where x_center and\n",
        "      y_center are the center coordinates of the bounding box relative to the\n",
        "      image dimensions, and width and height are the relative sizes of the\n",
        "      bounding box with respect to the image dimensions.\n",
        "\n",
        "      Args:\n",
        "          bbox (tuple): A tuple containing the x_min, y_min, width, and height\n",
        "                        of the bounding box in the format (x_min, y_min, width, height).\n",
        "          img_width (int): The width of the image in pixels.\n",
        "          img_height (int): The height of the image in pixels.\n",
        "\n",
        "      Returns:\n",
        "          list: A list containing the converted bounding box in YOLO format\n",
        "                [x_center, y_center, width, height], where all values are\n",
        "                relative to the image dimensions.\n",
        "    \"\"\" \n",
        "    x_min, y_min, width, height = bbox\n",
        "    x_center = (x_min + width / 2) / img_width\n",
        "    y_center = (y_min + height / 2) / img_height\n",
        "    width /= img_width\n",
        "    height /= img_height\n",
        "    return [x_center, y_center, width, height]"
      ],
      "metadata": {
        "id": "6UHYFzXAP2ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, annotation in enumerate(annotations['annotations']):\n",
        "\n",
        "    \"\"\"\n",
        "      Iterate through all annotations and split them into train and validation sets.\n",
        "      For each annotation, create the corresponding YOLO-format label files and copy\n",
        "      the image files to the appropriate train or validation folders.\n",
        "    \"\"\"\n",
        "    file_name = annotation['file_name']\n",
        "    image_path = os.path.join('../Data/cars', file_name)\n",
        "    img = cv2.imread(image_path)\n",
        "    img_height, img_width = img.shape[:2]\n",
        "\n",
        "    if i < train_size:\n",
        "        # Copy the image to the train folder\n",
        "        shutil.copy(image_path, os.path.join('../Data/car/train', file_name))\n",
        "\n",
        "        # Write the corresponding label file\n",
        "        with open(os.path.join('../Data/car/labels/train', file_name[:-4] + '.txt'), 'w') as label_file:\n",
        "            \"\"\"\n",
        "              Convert the bounding box to YOLO format and write it to the label\n",
        "              file for the training set.\n",
        "            \"\"\"\n",
        "            yolo_bbox = convert_bbox_to_yolo(annotation['bbox'], img_width, img_height)\n",
        "            label_file.write(f\"0 {' '.join(map(str, yolo_bbox))}\\n\")\n",
        "\n",
        "    else:\n",
        "        # Copy the image to the val folder\n",
        "        shutil.copy(image_path, os.path.join('../Data/car/val', file_name))\n",
        "\n",
        "        # Write the corresponding label file\n",
        "        with open(os.path.join('../Data/car/labels/val', file_name[:-4] + '.txt'), 'w') as label_file:\n",
        "            \"\"\"\n",
        "              Convert the bounding box to YOLO format and write it to the label\n",
        "              file for the validation set.\n",
        "            \"\"\"\n",
        "            yolo_bbox = convert_bbox_to_yolo(annotation['bbox'], img_width, img_height)\n",
        "            label_file.write(f\"0 {' '.join(map(str, yolo_bbox))}\\n\")"
      ],
      "metadata": {
        "id": "4EJxqFrBOcZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_content = \"\"\"\n",
        "train: ../Data/car/train\n",
        "val: ../Data/car/val\n",
        "\n",
        "nc: 1\n",
        "names: ['car']\n",
        "\"\"\"\n",
        "\n",
        "with open(\"car_dataset.yaml\", \"w\") as yaml_file:\n",
        "    yaml_file.write(yaml_content)"
      ],
      "metadata": {
        "id": "K_SqPTmvOkX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcF2Ynh9nIZ2",
        "outputId": "253f5d8b-6dbf-4909-f7a8-1418fa3354cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "benchmarks.py     \u001b[0m\u001b[01;34mdata\u001b[0m/       LICENSE          requirements.txt  tutorial.ipynb\n",
            "car_dataset.yaml  \u001b[01;34mData\u001b[0m/       \u001b[01;34mmodels\u001b[0m/          \u001b[01;34mruns\u001b[0m/             \u001b[01;34mutils\u001b[0m/\n",
            "CITATION.cff      detect.py   \u001b[01;34m__pycache__\u001b[0m/     \u001b[01;34msegment\u001b[0m/          val.py\n",
            "\u001b[01;34mclassify\u001b[0m/         export.py   README.md        setup.cfg         yolov5s.pt\n",
            "CONTRIBUTING.md   hubconf.py  README.zh-CN.md  train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training YOLOv5 on the Custom Car Dataset\n",
        "\n",
        "Now that I have prepared the custom car dataset in the required format, I can train YOLOv5 using this dataset. To do this, I will run the `train.py` script, providing the necessary arguments such as image size, batch size, number of epochs, dataset configuration file, pre-trained weights, and the name of the training run.\n"
      ],
      "metadata": {
        "id": "uy5MPtjpurtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** During the training process, I encountered an error like this:"
      ],
      "metadata": {
        "id": "ELcxCRAvvNkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This error indicates that the training script could not find any labels for the images in the training dataset. There might be several reasons for this issue, such as an incorrect path to the labels, improperly formatted label files, or missing label files. To resolve this issue, I double-checked the paths to the labels in the dataset configuration file and ensured that the label files were correctly formatted and located in the appropriate directories. For more information, consult the [YOLOv5 documentation on training custom data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data).\n"
      ],
      "metadata": {
        "id": "uIUojIXuvQDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the YOLOv5 training script with the following parameters:\n",
        "#   - Resize input images to 640x640 pixels (--img 640)\n",
        "#   - Set the batch size to 16 (--batch 16)\n",
        "#   - Train the model for 100 epochs (--epochs 100)\n",
        "#   - Use the configuration file 'car_dataset.yaml' to specify dataset and training parameters (--data car_dataset.yaml)\n",
        "#   - Initialize the model with pre-trained weights from 'yolov5s.pt' (--weights yolov5s.pt)\n",
        "#   - Name the output model and related files with the prefix 'car_detector' (--name car_detector)\n",
        "\n",
        "!python train.py --img 640 --batch 16 --epochs 100 --data car_dataset.yaml --weights yolov5s.pt --name car_detector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUSKbzYhR0jV",
        "outputId": "4fb6db6b-6540-4b9c-938a-28495feeab91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=car_dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=car_detector, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-120-g3e55763 Python-3.9.16 torch-1.13.1+cu116 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "2023-03-15 04:57:49.748201: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-15 04:57:51.169905: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-15 04:57:51.170105: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-15 04:57:51.170132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/Datrix/Data/car/train.cache... 0 images, 200 backgrounds, 0 corrupt: 100% 200/200 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gdrive/MyDrive/Datrix/yolov5/train.py\", line 640, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/gdrive/MyDrive/Datrix/yolov5/train.py\", line 529, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/content/gdrive/MyDrive/Datrix/yolov5/train.py\", line 188, in train\n",
            "    train_loader, dataset = create_dataloader(train_path,\n",
            "  File \"/content/gdrive/MyDrive/Datrix/yolov5/utils/dataloaders.py\", line 124, in create_dataloader\n",
            "    dataset = LoadImagesAndLabels(\n",
            "  File \"/content/gdrive/MyDrive/Datrix/yolov5/utils/dataloaders.py\", line 502, in __init__\n",
            "    assert nf > 0 or not augment, f'{prefix}No labels found in {cache_path}, can not start training. {HELP_URL}'\n",
            "AssertionError: \u001b[34m\u001b[1mtrain: \u001b[0mNo labels found in /content/gdrive/MyDrive/Datrix/Data/car/train.cache, can not start training. See https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validating the Trained YOLOv5 Model\n",
        "\n",
        "After training the YOLOv5 model on the custom car dataset, I will validate its performance using the validation dataset. To do this, I will run the `val.py` script, providing the necessary arguments such as dataset configuration file, trained weights, image size, IOU threshold, and whether to use half-precision inference.\n"
      ],
      "metadata": {
        "id": "0PWl7HDDuvz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the YOLOv5 validation script with the following parameters:\n",
        "#   - Use the configuration file 'car_dataset.yaml' to specify dataset and validation parameters (--data car_dataset.yaml)\n",
        "#   - Load the model weights from 'runs/train/car_detector/weights/best.pt' (--weights runs/train/car_detector/weights/best.pt)\n",
        "#   - Resize input images to 640x640 pixels (--img 640)\n",
        "#   - Set the Intersection over Union (IoU) threshold to 0.65 (--iou 0.65)\n",
        "#   - Use FP16 half-precision mode for faster inference and lower memory usage (--half)\n",
        "\n",
        "!python val.py --data car_dataset.yaml --weights runs/train/car_detector/weights/best.pt --img 640 --iou 0.65 --half"
      ],
      "metadata": {
        "id": "Q_cAHmjYR370",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b71bd15-8421-4489-e1ef-7d1f4f629e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=car_dataset.yaml, weights=['runs/train/car_detector/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 🚀 v7.0-120-g3e55763 Python-3.9.16 torch-1.13.1+cu116 CPU\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gdrive/MyDrive/Datrix/yolov5/val.py\", line 409, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/gdrive/MyDrive/Datrix/yolov5/val.py\", line 380, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/gdrive/MyDrive/Datrix/yolov5/val.py\", line 143, in run\n",
            "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
            "  File \"/content/gdrive/MyDrive/Datrix/yolov5/models/common.py\", line 344, in __init__\n",
            "    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n",
            "  File \"/content/gdrive/MyDrive/Datrix/yolov5/models/experimental.py\", line 79, in attempt_load\n",
            "    ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 771, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 270, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 251, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'runs/train/car_detector/weights/best.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5kiNO3VBylWQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}